{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Spark-RDD.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIANi-lsXlci"
      },
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_-oZKIGYMEq"
      },
      "source": [
        "1.\tCreate RDDs in three different ways."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLGEJPsRXlcp",
        "outputId": "d27054e4-0190-4adf-e899-08454a5b9d5c"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"application\").getOrCreate()\n",
        "type(spark)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.session.SparkSession"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKl_kIylXlcq"
      },
      "source": [
        "#1st method using parallelize\n",
        "text=\"My mission in life is not merely to survive but to thrive and to do so with some passion,some compassion, some humor, and some style.\".split(\" \")\n",
        "words = spark.sparkContext.parallelize(text,2)\n",
        "type(words)\n",
        "words.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRxTVrz-Xlcq",
        "outputId": "fd34a7b8-bd77-49b7-c736-d066d22a8085"
      },
      "source": [
        "#2nd method from data source\n",
        "rdd_2=spark.sparkContext.textFile(\"textfile.txt\")\n",
        "rdd_2.collect()\n",
        "type(rdd_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7XtJAVNXlcr"
      },
      "source": [
        "#3rd method pipelined RDD\n",
        "rdd_3=words.filter(lambda word:word.startswith('s'))\n",
        "rdd_3.collect()\n",
        "type(rdd_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmE_ZLq8YWTb"
      },
      "source": [
        "2.Read a text file and count the number of words in the file using RDD operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB7H6gw-Xlcr"
      },
      "source": [
        "word_count=rdd_2.flatMap(lambda s:s.split(\" \"))\n",
        "word_count.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diK1GMJljhSE"
      },
      "source": [
        "3.Write a program to find the word frequency in a given file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZQDyX_3Xlcs"
      },
      "source": [
        "info=spark.sparkContext.textFile(\"textfile.txt\")\n",
        "info=info.flatMap(lambda x:x.split())\n",
        "content_map=info.map(lambda c:(c,1))\n",
        "content_map.reduceByKey(lambda a,b:a+b).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVL5jUTAhjPn"
      },
      "source": [
        "4.Write a program to convert all words in a file to uppercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNYAVcR4Xlcs"
      },
      "source": [
        "rdd_2.map(lambda c:c.upper()).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlNtrhWGht_s"
      },
      "source": [
        "5.Write a program to convert all words in a file to lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PksvvrYXlct"
      },
      "source": [
        "rdd_2.map(lambda c:c.lower()).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXbt1dIXhxrv"
      },
      "source": [
        "6.Write a program to capitalize first letter of each words in file (use string capitalize() method)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQS76SchXlcu"
      },
      "source": [
        "capital=rdd_2.flatMap(lambda a:a.split(\" \")).map(lambda c:c.capitalize()).collect()\n",
        "\" \".join(capital)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLIoB6U6h2XU"
      },
      "source": [
        "7.Find the longest length of word from given set of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DtZo3JIXlcu"
      },
      "source": [
        "longest_word=rdd_2.flatMap(lambda x:x.split(\" \"))\n",
        "longest_word.map(lambda nu:(len(nu),nu)).max()[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQTp3qENh9_G"
      },
      "source": [
        "8.Map the Registration numbers to corresponding branch. 6000 series ML, 9000 series HAD, 1000 series MS, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pILcxv3aXlcu"
      },
      "source": [
        "registration_number=[6027,2005,2035,6011,9007,9056,3088,3045,4088,4065,5077,5066,1001,1002]\n",
        "context=spark.sparkContext.parallelize(registration_number,2)\n",
        "classify=context.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
        "        else ('MS',reg) if reg>1000 and reg<2000\n",
        "        else ('ES',reg) if reg>3000 and reg<4000\n",
        "        else ('MSc',reg) if reg>4000 and reg<5000\n",
        "        else ('CC',reg) if reg>5000 and reg<6000\n",
        "        else ('ML',reg) if reg>6000 and reg<7000\n",
        "        else ('HDA',reg))\n",
        "classify.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjopdslQiN7d"
      },
      "source": [
        "9.Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDOG_0R1Xlcv"
      },
      "source": [
        "reg_no=[6027]\n",
        "info=spark.sparkContext.parallelize(reg_no,2)\n",
        "ans=info.map(lambda reg:('VLSI',reg) if reg>2000 and reg<3000 \n",
        "        else ('MS',reg) if reg>1000 and reg<2000\n",
        "        else ('ES',reg) if reg>3000 and reg<4000\n",
        "        else ('MSc',reg) if reg>4000 and reg<5000\n",
        "        else ('CC',reg) if reg>5000 and reg<6000\n",
        "        else ('ML',reg) if reg>6000 and reg<7000\n",
        "        else ('HDA',reg))\n",
        "ans.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc1n1oWnXlcv"
      },
      "source": [
        "'''10.\tA text file contains data about citizens of country. \n",
        "Fields(information in file) are Name, dob, Phone, email and state name. \n",
        "Another file contains mapping of state names to state code like Karnataka is codes as KA, \n",
        "TamilNadu as TN, Kerala KL etc. Compress the file will by changing full state name to state code.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVfJL2dXXlcw"
      },
      "source": [
        "details=spark.sparkContext.textFile(\"user_details.txt\")\n",
        "code=spark.sparkContext.textFile(\"state_codes_of_different_states.txt\")\n",
        "details_rdd=details.map(lambda x:x.split(\",\")).collect()\n",
        "code_rdd=code.map(lambda y:y.split(\",\")).collect()\n",
        "for i in range(len(details_rdd)):\n",
        "    for j in range(len(code_rdd)):  \n",
        "        if details_rdd[i][4]==code_rdd[j][0]:\n",
        "            details_rdd[i][4]=code_rdd[j][1]\n",
        "details_rdd              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF2prHqaXlcw"
      },
      "source": [
        "stRDD = spark.sparkContext.textFile('state_codes_of_different_states.txt')\n",
        "stateKey = stRDD.map(lambda word: (word.split(',')[0], word.split(',')[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxjin8QcXlcw",
        "outputId": "23bccde3-d043-4d23-aeeb-e076cdcfef54"
      },
      "source": [
        "#creating dictionary\n",
        "code_dict = {}\n",
        "for val in stateKey.collect():\n",
        "    code_dict[val[0]] = val[1]\n",
        "    \n",
        "code_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Karnataka': 'KA',\n",
              " 'JammuKashmir': 'JK',\n",
              " 'Gujarat': 'GJ',\n",
              " 'Maharashtra': 'MH',\n",
              " 'Rajasthan': 'RJ',\n",
              " 'Punjab': 'PB',\n",
              " 'AndhraPradesh': 'AP',\n",
              " 'TamilNadu': 'TN',\n",
              " 'UttarPradesh': 'UP',\n",
              " 'WestBengal': 'WB'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgycL2QJXlcw"
      },
      "source": [
        "mapData = spark.sparkContext.broadcast(code_dict)\n",
        "\n",
        "cityRdd = spark.sparkContext.textFile('user_details.txt')\n",
        "print(cityRdd.collect())\n",
        "def abc(state,codes):\n",
        "    splitData = state.split(',')  \n",
        "    print(splitData)\n",
        "    splitData[4] = codes.value.get(splitData[4])\n",
        "    newData = ' '\n",
        "    newData = newData.join(splitData)\n",
        "    \n",
        "    return newData\n",
        "    \n",
        "mapCitizen = cityRdd.map(lambda data: abc(data,mapData))\n",
        "mapCitizen.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG7jgOBtXlcx"
      },
      "source": [
        "'''11.Text file contain numbers. Numbers are separated by one white space. \n",
        "There is no order to store the numbers. One line may contain one or more numbers.\n",
        "Find the maximum, minimum, sum and mean of numbers.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vpR_3U7Xlcx"
      },
      "source": [
        "file1=spark.sparkContext.textFile(\"no..txt\")\n",
        "file1_rdd=file1.flatMap(lambda z:z.split(\" \")).map(lambda c:int(c))\n",
        "file1_rdd.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y4wC5pbXlcx"
      },
      "source": [
        "file1_rdd.min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4u7TkDAXlcx"
      },
      "source": [
        "file1_rdd.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9a2B6JOXlcx"
      },
      "source": [
        "file1_rdd.mean()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}